=====================
 PyPy - Translation
=====================

.. contents::
.. sectnum::

This document describes the tool chain that we developed to analyze and
"compile" RPython_ programs (like PyPy itself) to various lower-level
languages.

.. _RPython: coding-guide.html#restricted-python


Overview
========

XXX very preliminary documentation!

The module `translator.py`_ is the common entry point to the various parts
of the translation process.  It is available as an interactive utility to
`play around`_.

Here are the steps we follow to translate a given program:

1. The complete program is imported.  If needed, extra initialization is performed.  Once this is done, the program must be present in memory is a form that is "static enough" in the sense of RPython_.

2. The `Flow Object Space`_ processes the input program, turning each function independently into a `control flow graph`_ data structure recording sequences of basic operations in "single-style assignment".

3. Optionally, the Annotator_ performs global type inference on the control flow graphs.  Each variable gets annotated with an inferred type.

4. The `RPython typer`_ can use the high-level types inferred by the Annotator to turn the operations in the control flow graphs into low-level operations over low-level types (close to the C types: struct, array, pointer...).

5. One of the Code Generators (XXX not documented yet) turns the optionally annotated/typed flow graphs and produces a source file in a lower-level language: C_, LLVM_, `Common Lisp`_, Pyrex_, Java_, or `Python again`_ (this is used in PyPy to turn sufficiently RPythonic app-level code into interp-level code).

6. This lower-level source file is compiled to produce an executable.

.. _`translator.py`: http://codespeak.net/svn/pypy/dist/pypy/translator/translator.py
.. _`play around`: getting_started.html#trying-out-the-translator
.. _`Flow Object Space`: objspace.html#the-flow-object-space
.. _`control flow graph`: objspace.html#the-flow-model
.. _`Common Lisp`: http://codespeak.net/svn/pypy/dist/pypy/translator/gencl.py
.. _Pyrex: http://codespeak.net/svn/pypy/dist/pypy/translator/pyrex/genpyrex.py
.. _Java: http://codespeak.net/svn/pypy/dist/pypy/translator/java/



.. _Annotator:

The annotation pass
===================

(INCOMPLETE DRAFT)

We describe below how a control flow graph can be "annotated" to 
discover the types of the objects.  This annotation pass is a form of 
type inference.  It is done after control flow graphs are built by the 
FlowObjSpace, but before these graphs are translated into low-level code 
(e.g. C/Lisp/Pyrex).


Model
------------------------

The major goal of the annotator is to "annotate" each variable that 
appears in a flow graph.  An "annotation" describes all the possible 
Python objects that this variable could contain at run-time, based on a 
whole-program analysis of all the flow graphs --- one per function.

An "annotation" is an instance of ``SomeObject``.  There are subclasses 
that are meant to represent specific families of objects.  Note that 
these classes are all meant to be instantiated; the classes ``SomeXxx`` 
themselves are not the annotations.

Here is an overview (see ``pypy.annotation.model``):

* ``SomeObject`` is the base class.  An instance ``SomeObject()`` 
  represents any Python object.  It is used for the case where we don't 
  have enough information to be more precise.  In practice, the presence 
  of ``SomeObject()`` means that we have to make the annotated source code 
  simpler or the annotator smarter.

* ``SomeInteger()`` represents any integer.  
  ``SomeInteger(nonneg=True)`` represent a non-negative integer (``>=0``).

* ``SomeString()`` represents any string; ``SomeChar()`` a string of 
  length 1.

* ``SomeTuple([s1,s2,..,sn])`` represents a tuple of length ``n``.  The 
  elements in this tuple are themselves constrained by the given list of 
  annotations.  For example, ``SomeTuple([SomeInteger(), SomeString()])`` 
  represents a tuple with two items: an integer and a string.

There are more complex subclasses of ``SomeObject`` that we describe in 
more details below.

All the ``SomeXxx`` instances can optionally have a ``const`` attribute, 
which means that we know exactly which Python object the Variable will 
contain.

All the ``SomeXxx`` instances are supposed to be immutable.  The 
annotator manages a dictionary mapping Variables (which appear in flow 
graphs) to ``SomeXxx`` instances; if it needs to revise its belief about 
what a Variable can contain, it does so by updating this dictionary, not 
the ``SomeXxx`` instance.



Annotator
--------------------------

The annotator itself (``pypy.translator.annrpython``) works by 
propagating the annotations forward in the flow graphs, starting at some 
entry point function, possibly with explicitely provided annotations 
about the entry point's input arguments.  It considers each operation in 
the flow graph in turn.  Each operation takes a few input arguments 
(Variables and Constants) and produce a single result (a Variable).  
Depending on the input argument's annotations, an annotation about the 
operation result is produced.  The exact rules to do this are provided 
by the whole ``pypy.annotation`` subdirectory, which defines all the 
cases in detail according to the R-Python semantics.  For example, if 
the operation is 'v3=add(v1,v2)' and the Variables v1 and v2 are 
annotated with ``SomeInteger()``, then v3 also receives the annotation 
``SomeInteger()``.  So for example the function::

    def f(n):
        return n+1

corresponds to the flow graph::

    start ----------.
                    |
                    V 
           +-------------------+
           |  v2 = add(v1, 1)  |
           +-------------------+
                    |
                    `---> return block

If the annotator is told that v1 is ``SomeInteger()``, then it will 
deduce that v2 (and hence the function's return value) is 
``SomeInteger()``.

.. _above:

This step-by-step annotation phase proceeds through all the operations 
in a block, and then along the links between the blocks of the flow 
graph.  If there are loops in the flow graph, then the links will close 
back to already-seen blocks, as in::

    def g(n):
        i = 0
        while n:
            i = i + n
            n = n - 1
        return i

whose flow graph is::

    start -----.
               | n1 0
               V
           +-------------------+
           |   input: n2 i2    |
           |  v2 = is_true(n2) | <-----------.
           +-------------------+       m3 j3 |
               |             |               |
               |ifFalse      |ifTrue         |
    return <---' i2          | n2 i2         |
                             V               |
                    +--------------------+   |
                    |   input: n3 i3     |   |
                    |  j3 = add(i3, n3)  |   |
                    |  m3 = sub(n3, 1)   |---'
                    +--------------------+

Be sure to follow the variable renaming that occurs systematically 
across each link in a flow graph.  In the above example the Variables 
have been given names similar to the name of the original variables in 
the source code (the FlowObjSpace tries to do this too) but keep in mind 
that all Variables are different: n1, n2, i2, v2, n3, i3, j3, m3.

Assume that we call the annotator with an input annotation of 
``SomeInteger()`` for n1.  Following the links from the start, the 
annotator will first believe that the Variable i2, whose value comes 
from the constant 0 of the first link, must always be zero.  It will 
thus use the annotation ``SomeInteger(const=0)`` for i2.  Then it will 
propagate the annotations through both blocks, and find that v2 is 
``SomeBool()`` and all other variables are ``SomeInteger()``.  In 
particular, the annotation of j3 is different from the annotation of the 
Variable i2 into which it is copied (via the back-link).  More 
precisely, j3 is ``SomeInteger()`` but i2 is the more specific 
``SomeInteger(const=0)``.  This means that the assumption that i2 must 
always be zero is found to be wrong.  At this point, the annotation of 
i2 is *generalized* to include both the existing and the new annotation.  
(This is the purpose of ``pypy.annotation.model.unionof()``).  Then 
these more general annotations must again be propagated forward.

This process of successive generalizations continues until the 
annotations stabilize.  In the above example, it is sufficient to 
re-analyse the first block once, but in general it can take several 
iterations to reach a fixpoint.  Annotations may also be propagated from 
one flow graph to another and back repeatedly, across ``call`` 
operations.  The overall model should ensure that this process 
eventually terminates under reasonable conditions.  Note that as long as 
the process is not finished, the annotations given to the Variables are 
wrong, in the sense that they are too specific; at run-time, the 
Variables will possibly contain Python objects outside the set defined 
by the annotation, and the annotator doesn't know it yet.


Description of the available types
-----------------------------------------------

The reference and the details for the annotation model is found in the 
module ``pypy.annotation.model``.  We describe below the issues related 
to the various kinds of annotations.


Simple Types
++++++++++++

``SomeInteger``, ``SomeBool``, ``SomeString``, ``SomeChar`` all stands 
for the obvious corresponding set of immutable Python objects.


Tuples
++++++

``SomeTuple`` only considers tuples of known length.  We don't try to 
handle tuples of varying length (the program should use lists instead).


Lists and Dictionaries
++++++++++++++++++++++

``SomeList`` stands for a list of homogeneous type (i.e. all the
elements of the list are represented by a single common ``SomeXxx``
annotation).

``SomeDict`` stands for a homogeneous dictionary (i.e. all keys have
the same ``SomeXxx`` annotation, and so have all values).

These types are mutable, which requires special support for the
annotator.  The problem is that in code like::

   lst = [42]
   update_list(lst)
   value = lst[0]

the annotation given to ``value`` depends on the order in which the
annotator progresses.  As ``lst`` is originally considered as a list
of ``SomeInteger(const=42)``, it is possible that ``value`` becomes
``SomeInteger(const=42)`` as well if the analysis of ``update_list()``
is not completed by the time the third operation is first considered.


To solve this problem, each ``SomeList`` or ``SomeDict`` is linked to
so-called *list-definition* or *dict-definition*
(``pypy.annotation.listdef.ListDef``,
``pypy.annotation.dictdef.DictDef``).  The list definitions and dict
definitions contain information about the type of list items, dict
keys and values respectively.

At each creation point, i.e. each 'newlist' or 'newdict', a
``SomeList`` or ``SomeDict`` is created with a fresh definition object
if its the first time we annotate this operation, otherwise the
definition setup (which has been cached) the first time is reused.
While proceeding the annotator also records on the definition objects
all flow graph positions where values are read from the list or dict.

For example, in code like::

   lst = [42]
   f(lst[0])
   lst.append(43)

the definition associated with the list at creation time (first line)
represents list whose items are all constants equal to 42; when a
value is read from the list (second line) this position is recorded on
the definition; when the ``append(43)`` call is then found, the item
type information in the definition is generalized (in this case to
general list of integers) and the annotator schedule all the so far
recorded read positions for reflowing, in order to keep the
annotations consistent.

Our model is not sensitive to timing: it doesn't know that the same
list object may contain different items at different times.  It only
computes how general the items in the list must be to cover all cases.

For initially empty lists, as created by ``lst = []``, we build a list
whose items have the annotation ``SomeImpossibleValue``.  This is an
annotation that denotes that no Python object at all can possibly
appear here at run-time.  It is the least general annotation.  The
rationale is that::

   lst = []
   oups = lst[0]

will give the variable ``oups`` the annotation
``SomeImpossibleValue``, which is reasonable given that no concrete
Python object can ever be put in ``oups`` at run-time.  In a more
usual example::

   lst = []
   lst.append(42)

the list is first built with ``SomeImpossibleValue`` items, and then
the factory is generalized to produce a list of
``SomeInteger(const=42)``.  With this "impossible object" trick we
don't have to do anything special about empty lists.

When the annotator has to unify different list or dict annotations it
effectively unifies the corresponding definitions, the item types are
generalized as necessary and the union of the read position sets is
used, also things are internally setup in such a way that the involved
definitions now are considered interchangeably the same for the rest of
the process (that means that union for lists and dicts is a not
reversible operation).  If the new item type is more general reflowing
from all the read positions is also scheduled.

User-defined Classes and Instances
++++++++++++++++++++++++++++++++++

``SomeInstance`` stands for an instance of the given class or any 
subclass of it.  For each user-defined class seen by the annotator, we 
maintain a ClassDef (``pypy.annotation.classdef``) describing the 
attributes of the instances of the class; essentially, a ClassDef gives 
the set of all class-level and instance-level attributes, and for each 
one, a corresponding ``SomeXxx`` annotation.

Instance-level attributes are discovered progressively as the annotation 
progresses.  Assignments like::

   inst.attr = value

update the ClassDef of the given instance to record that the given 
attribute exists and can be as general as the given value.

For every attribute, the ClassDef also records all the positions where 
the attribute is *read*.  If, at some later time, we discover an 
assignment that forces the annotation about the attribute to be 
generalized, then all the places that read the attribute so far are 
marked as invalid and the annotator will have to restart its analysis 
from there.

The distinction between instance-level and class-level attributes is 
thin; class-level attributes are essentially considered as initial 
values for instance-level attributes.  Methods are not special in this 
respect, expect that they are bound to the instance (i.e. ``self = 
SomeInstance(cls)``) when considered as the initial value for the 
instance.

The inheritance rules are as follows: the union of two ``SomeInstance`` 
annotations is the ``SomeInstance`` of the most precise common base 
class.  If an attribute is considered (i.e. read or written) through a 
``SomeInstance`` of a parent class, then we assume that all subclasses 
also have the same attribute, and that the same annotation applies to 
them all (so code like ``return self.x`` in a method of a parent class 
forces the parent class and all its subclasses to have an attribute 
``x``, whose annotation is general enough to contain all the values that 
all the subclasses might want to store in ``x``).  However, distinct 
subclasses can have attributes of the same names with different, 
unrelated annotations if they are not used in a general way through the 
parent class.


Prebuilt Constants and instance methods
+++++++++++++++++++++++++++++++++++++++

Constants in the flowgraph are annotated with a corresponding
``SomeXxx`` instance with 'const' attribute set to the their value.

Constant instances of user-defined classes, callables (which include
functions but also class types themself) and staticmethod are treated
specially.  Constant user-defined class instances can declare themself
immutable by having a '_freeze_' method returning true, otherwise they
will be assumed mutable and be annotated with usual ``SomeInstance``
annotation without 'const' set.

For user-defined constant instances that declared themself immutable,
staticmethods and other callables ``SomePBC`` is used (PBC = pre-built
constant). Its instances contain a 'prebuiltinstances' dictionary. For
the normal case and single value ``x`` this will be set to ``{x :
True}``. For a single value the 'const' attribute will also be set.

The union of ``SomePBC`` instances will result in an instance with the
merge of the original dictionaries.  So for example a dictionary
pointing to functions, will usually have as its value annotation such
a ``SomePBC`` with a 'prebuiltinstances' dict having all the functions
as keys.

For a large part of operations when encountering ``SomeXxx`` with
'const' set the annotator will do constant propagation and produce
results with also 'const' set. This also means that based on 'const'
truth values the annotator will not flow into code that is not
reachable given global constant values. A later graph transformation
will remove such dead code.

XXX None, how methods annotation storage work


XXX complete


Built-in functions and methods
++++++++++++++++++++++++++++++

(to be completed)


Others
++++++

(to be completed)




.. _`RPython typer`:

The RPython Typer
=================

http://codespeak.net/svn/pypy/dist/pypy/rpython/


Overview
--------

The RPython Typer is the bridge between the Annotator_ and the low-level code generators.  The annotator computes types (or "annotations") that are high-level, in the sense that they describe RPython types like lists or instances of user-defined classes.  In general, though, to emit code we need to represent these high-level annotations into the low-level model of the target language; for C, this means structures and pointers and arrays.  The Typer both determines the appropriate low-level type for each annotation, and tries to replace *all* operations in the control flow graphs with one or a few low-level operations.  Just like low-level types, there is only a fairly restricted set of low-level operations, along the lines of reading or writing from or to a field of a structure.

In theory, this step is optional; some code generators might be able to read directly the high-level types.  However, we expect that case to be the exception.  "Compiling" high-level types into low-level ones is rather more messy than one would expect.  This was the motivation for making this step explicit and isolated in a single place.  After Typing, the graphs can only contain very few operations, which makes the job of the code generators much simpler.


Example: Integer operations
---------------------------

Integer operations are the easiest.  Assume a graph containing the following operation::

    v3 = add(v1, v2)

annotated::

    v1 -> SomeInteger()
    v2 -> SomeInteger()
    v3 -> SomeInteger()

then obviously we want to type it and replace it with::

    v3 = int_add(v1, v2)

where -- in C notation -- all three variables v1, v2 and v3 are typed ``int``.  This is done by attaching an attribute ``concretetype`` to v1, v2 and v3 (which might be instances of Variable or possibly Constant).  In our model, this ``concretetype`` is ``pypy.rpython.lltypes.Signed``.  Of course, the purpose of replacing the operation called ``add`` with ``int_add`` is that code generators no longer have to worry about what kind of addition (or concatenation maybe?) it means.


The process in more details
---------------------------

The RPython Typer does the following transformations for each block of all the annotated flow graphs (each block is processed independently, by assuming that the already-computed annotations are globally correct):

* We first replace all Variables that have constant annotations with real Constants in the flow graph.

* For the time being, we assume that each SomeXxx annotation has a canonical low-level representation.  For example, all variables annotated with SomeInteger() will correspond to the ``Signed`` low-level type.  Each input argument of the block are tagged with the canonical low-level representation (this is done by attaching an attribute ``concretetype`` to each Variable).

* Each operation, with its argument's annotation, is looked up in a table which specifies with which low-level operation(s) it should be substituted.  If needed, the arguments are first converted (with extra operations) from their current ``concretetype`` to the required low-level types.  For constant arguments, we just attach the ``concretetype`` to the Constant instance; as for Variables, this tells the code generator of which type the constant really is.  Finally, the substitution rules specify the ``concretetype`` of the result.  It is attached to the result Variable, and will be used further down the block to detect when conversions are needed.

* When a block has been transformed in this way, all the links are considered; if the concrete types of the Variables that exit do not match the canonical low-level types expected by the target block, conversions are inserted -- they are put in a new block inserted along the link, as they are of no concern to the other exit links.

This may look like flowing, similar to what the annotator does, but it is limited to a single block; for global coherency it trusts the more involved fixpoint-based algorithm run by the annotator.


Low-Level Types
---------------

For now, the RPython Typer uses a standard low-level model which we believe can correspond rather directly to various target languages from C to LLVM to Java.  This model is implemented in the first part of `lltypes.py`_.

The second part of `lltypes.py`_ is a runnable implementation of these types, for testing purposes.  It allows us to write and test plain Python code using a malloc() function to obtain and manipulate structures and arrays.  This is useful for example to implement RPython types like 'list' with its operations and methods.

The basic assumption is that Variables (i.e. local variables and function arguments and return value) all contain "simple" values: basically, just integers or pointers.  All the "container" data structures (struct and array) are allocated in the heap, and they are always manipulated via pointers.  (There is no equivalent to the C notion of local variable of a ``struct`` type.)

.. _`lltypes.py`: http://codespeak.net/svn/pypy/dist/pypy/rpython/lltypes.py


Primitive Types
+++++++++++++++

Signed
    a signed integer in one machine word (a ``long``, in C)

Unsigned
    a non-signed integer in one machine word (``unsigned long``)

Char
    a single character (``char``)

Bool
    a boolean value (could be ``char`` as well in C)

Void
    a constant.  Meant for variables and function arguments that should
    disappear from the generated code.


Structure Types
+++++++++++++++

Structure types are built as instances of ``pypy.rpython.lltypes.Struct``::

    MyStructType = Struct('somename',  ('field1', Type1), ('field2', Type2)...)

This declares a structure (or a Pascal ``record``) containing the specified named fields with the given types.  The field names cannot start with an underscore.  As noted above, you cannot directly manipulate structure objects, but only pointer to structures living in the heap.

By contrast, the fields themselves can be of primitive, pointer or container type.  When a structure contains another structure as a field we say that the latter is "inlined" in the former: the bigger structure contains the smaller one as part of its memory layout.

A structure can also contain an inlined array (see below), but only as its last field: in this case it is a "variable-sized" structure, whose memory layout starts with the non-variable fields and ends with a variable number of array items.  This number is determined when a structure is allocated in the heap.  Variable-sized structures cannot be inlined in other structures.


Array Types
+++++++++++

An array type is built as an instance of ``pypy.rpython.lltypes.Array``::

    MyArrayType = Array(('field1', Type1), ('field2', Type2)...)

The items of an array are always structures; the arguments to Array() give the fields of these structures (it can of course be a single field).  The allowed field types follow the same rules as for Struct(), but this particular structure cannot be variable-sized.

For now, each array stores its length explicitely in a header.  An array can never be resized: it occupies a fixed amount of bytes determined when it is allocated.


Pointer Types
+++++++++++++

As in C, pointers provide the indirection needed to make a reference modifiable or sharable.  Pointers can only point to a structure, an array, a function (see below) or a PyObject (see below).  Pointers to primitive types, if needed, must be done by pointing to a structure with a single field of the required type.  Pointer types are declared using one of::

   GcPtr(T, **flags)
   NonGcPtr(T, **flags)

The so-called GC pointers are the ones that hold a reference to the object they point to.  Typically, the malloc() operation allocates and returns a GcPtr to a new structure or array.  In a refcounting implementation, malloc() would allocate enough space for a reference counter before the actual structure, and initialize it to 1.  Actually, GC pointers can only point to a malloc()ed structure or array.  Non-GC pointers are used when you know that a pointer doesn't hold a (counted) reference to an object, usually because the object has no reference counter at all: for example, functions don't have one; more importantly, inlined substructures don't have one either.  For them, care must be taken to ensure that the bigger structure of which they are part of isn't freed while the NonGcPtr to the substructure is still in use.

All pointer types can also have additional flags, whose meaning is unspecified at this level (apart from the flag ``gc=True`` which GcPtrs have and NonGcPtrs miss).  Types with different flags are incompatible, but the cast_flags() operation is provided to perform explicit casts.  The intention is for example to represent the high-level object "the method append() of this list" as the type ``GcPtr(ListType, method='append')`` -- i.e. a pointer to the list in question with an additional flag specifying that the pointer represents the method append() of that list, as opposed to the list itself.


Function Types
++++++++++++++

The declaration::

    MyFuncType = FuncType([Type1, Type2, ...], ResultType)

declares a function type taking arguments of the given types and returning a result of the given type.  All these types must be primitives or pointers.  The function type itself is considered to be a "container" type: if you wish, a function contains the bytes that make up its executable code.  As with structures and arrays, they can only be manipulated through pointers.  More precisely, the only thing you can do -- more or less -- with MyFuncType is to embbed it in a NonGcPtr.


The PyObject Type
+++++++++++++++++

This is a special type, for compatibility with CPython: it stands for a structure compatible with PyObject.  It should be manipulated via GcPtr or NonGcPtr, depending on whether the CPython reference counter should be updated or not.  A typed graph can still contain generic space operations (add, getitem, etc.) provided they are applied on objects whose low-level type is a pointer to ``PyObject``.  In fact, code generators that support this should consider that the default type of a variable, if none is specified, is ``GcPtr(PyObject)``.  In this way, they can generate the correct code for fully-untyped flow graphs.


Implementing RPython types
--------------------------

As hinted above, the RPython types (e.g. 'list') are implemented in some "restricted-restricted Python" format by manipulating only low-level types, as provided by the "testing" concrete implementation of malloc() and friends.  What occurs then is that the same (tested!) very-low-level Python code -- which looks really just like C -- is then transformed into a flow graph and integrated with the rest of the user program.  In other words, we replace an operation like ``add`` between two variables annotated as SomeList, with a ``simple_call`` operation invoking this very-low-level list concatenation.

This list concatenation flow graph is then annotated as usual, with one difference: the annotator has to be taught about malloc() and the way the pointer thus obtained can be manipulated.  This generates a flow graph which is hopefully completely annotated with the SomePtr annotation.  Introduced just for this case, SomePtr maps directly to a low-level pointer type.  This is the only change needed to the Annotator to allow it to perform type inferrence of our very-low-level snippets of code.

See for example http://codespeak.net/svn/pypy/dist/pypy/rpython/rlist.py.



.. _C:
.. _GenC:

The C Back-End
==============

http://codespeak.net/svn/pypy/dist/pypy/translator/genc/


Overview
--------

The task of GenC is to convert a flow graph into C code.  By itself, GenC does not use the annotations in the graph.  It can actually convert unannotated graphs to C.  However, to make use of the annotations if they are present, an extra pass is needed: the `RPython Typer`_, whose task is to modify the flow graph according to the annotations, replacing operations with lower-level C-ish equivalents.

XXX GenC is currently in the process of being updated to use the RPython Typer. more documentation needed when this is done. But the basic principle of creating code from flowgraphs is similar to the `Python back-end`_.



.. _LLVM:

The LLVM Back-End
=================

http://codespeak.net/svn/pypy/dist/pypy/translator/llvm/


Overview
--------

Note that the LLVM back-end is in a state of constant flux and thus this are
preliminary notes only.

The task of GenLLVM is to convert a flow graph into `LLVM code`_, which can
then be optimized and compiled by LLVM. GenLLVM depends heavily on the
annotations, functions without annotations cannot be translated. The flowgraph
is not changed by GenLLVM in contrast to GenC. After the generation and
compilation of the LLVM code a wrapper is generated (at the moment with the
help of Pyrex) wich performs the conversion of arguments and return value of
the entry function to the types used by LLVM. Thus it is possible to call the
entry function from Python.

GenLLVM does not depend on the CPython runtime which has the drawback that most
functions with SomeObject annotations cannot be compiled properly -- the only
operations that are allowed on variables with SomeObject annotations are
``isinstance`` and ``type``.

GenLLVM creates for every object in the flow graph (e.g. constants, variables,
blocks...) an LLVM 'representation'. This representation knows how to
represent the corresponding object in LLVM and knows what code to generate for
space operations on the object, what global definitions the object needs etc.

Some examples to make this clearer: A `ClassRepr`_ object represents a class, a
`FuncRepr`_ object represent a function (or method). The following happens if
the space operation ``simple_call`` is performed on a function: An appropriate
``FuncRepr`` object is constructed which generates LLVM code for the function
it represents. Then the ``FuncRepr`` inserts the appropriate LLVM instructions
into the LLVM code of the function it is called from (sometime this is more
than just a call: the arguments have to be casted, etc). Something similar
happens if a class is instantiated: A ``ClassRepr`` is created which then
generates LLVM code that allocates enough memory for an instance of the class
and then (if the class or a base class has an ``__init__`` method) tells the
``FuncRepr`` of the appropriate ``__init__`` method to generate the code for
the call to it. 

Every representation object has a some other representations it depends on: A
``ListRepr`` of lists instances of a class depends on the ``ClassRepr`` of
that class. To ensure that the typedef of of the class is written to the llvm
file before the typedef of the list, the dependency tree of representations
traversed depth first when the LLVM code is written to a file.

.. _`LLVM code`: http://www.llvm.org
.. _`ClassRepr`: http://codespeak.net/svn/pypy/dist/pypy/translator/llvm/classrepr.py
.. _`FuncRepr`: http://codespeak.net/svn/pypy/dist/pypy/translator/llvm/funcrepr.py


Details about the representations
---------------------------------

Simple representations
++++++++++++++++++++++

There are some objects that have direct counterparts in LLVM: ints, floats,
chars (strings of length 1), bools. Most space operations involving those are
implemented as `tiny function`_ (LLVM doesn't support macros since LLVM's .ll
files correspond directly to its bytecode format so that round trip
conversions are nearly lossless). These are so simple that they are always
inlined by the LLVM optimizer so this doesn't lead to any performance penalty.


Function representation
+++++++++++++++++++++++

The representation of function in LLVM code is relatively easy since LLVM as
well as flow graph use SSA form. Furthermore LLVM supports exactly the kind of
control structures that the flow graphs feature: A function consists of basic
blocks that end with links to other blocks, data flows along these links. The
data flow is handled in LLVM by phi nodes: at the beginning of every block phi
nodes may be inserted. Those determine the value of a variable depending on
which block branched to the currect block. Example::

    block1:
        %b = phi int [1, %block0], [2, %block2]

Here %b is 1 if control came from block0 and 2 if control came from block2.

The following code is generated for the function ``g`` defined above_::

    int %g(int %n1) {
    block0:
        br label %block1
    block1:
        %n2 = phi int [%n1, %block0], [%m3, %block3]
        %i2 = phi int [0, %block0], [%j3, %block3]
        %v2 = call bool %std.is_true(int %n2)
        br bool %v2, label %block3, label %block2
    block2:
        %i4 = phi int [%i2, %block1]
        ret int %i4
    block3:
        %n3 = phi int [%n2, %block1]
        %i3 = phi int [%i2, %block1]
        %j3 = call int %std.add(int %i3, int %n3)
        %m3 = call int %std.sub(int %n3, int 1)
        br label %block1
    }

Note how the phi nodes correspond to the links in the control flow graph.

List representation
+++++++++++++++++++

Lists are represented as arrays. The code for the basic operation on lists
(``getitem``, ``setitem``, ``add``, ``mul``, ``append``, ``pop``...) is
`written in C`_. This C code is then compiled to LLVM code with the help of
the LLVM C-front-end. The resulting LLVM code is then transformed (with search
and replace) to fit in with the rest of GenLLVM. To support lists with
different types of items the C code implements lists as arrays of pointers to
``item``, where ``item`` is a dummy struct that is replaced with whatever type
is wanted.


XXX More to come.



.. _`tiny function`: http://codespeak.net/svn/pypy/dist/pypy/translator/llvm/operations.ll
.. _`written in C`: http://codespeak.net/svn/pypy/dist/pypy/translator/llvm/list.c

.. _`Python again`:
.. _`Python back-end`:

The Interplevel Back-End
========================

http://codespeak.net/svn/pypy/dist/pypy/translator/geninterplevel.py

Motivation
----------

PyPy often makes use of `application-level`_ helper methods.  
The idea of the 'geninterplevel' backend is to automatically transform 
such application level implementations to their equivalent representation 
at interpreter level.  Then, the RPython to C translation hopefully can 
produce more efficient code than always re-interpreting these methods.

One property of translation from application level Python to
Python is, that the produced code does the same thing as the
corresponding interpreted code, but no interpreter is needed
any longer to execute this code.

.. _`application-level`: architecture.html#app-preferable
.. _exceptions: http://codespeak.net/svn/pypy/dist/pypy/lib/_exceptions.py
.. _oldstyle: http://codespeak.net/svn/pypy/dist/pypy/lib/_classobj.py

Examples are exceptions_ and oldstyle_ classes. They are
needed in a very early phase of bootstrapping StdObjspace, but
for simplicity, they are written as RPythonic application
level code. This implies that the interpreter must be quite
completely initialized to execute this code, which is
impossible in the early phase, where we have neither
exceptions implemented nor classes available.

Solution
--------

This bootstrap issue is solved by invoking a new interpreter which
runs on FlowObjspace. FlowObjspace is complete without complicated
initialization. It is able to do abstract interpretation of any
Rpythonic code, without actually implementing anything. It just
records all the operations the interpreter would have done by
building flowgraphs for all the code. What the Python backend does is
just to produce correct Python code from these flowgraphs and return
it as source code.

Example
-------

.. _implementation: http://codespeak.net/svn/pypy/dist/pypy/translator/geninterplevel.py

Let's try the little example from above_. You might want to look at the 
flowgraph that it produces. Here, we directly run the Python translation 
and look at the generated source. See also the header section of the implementation_ 
for the interface::

    >>> from pypy.translator.geninterplevel import translate_as_module
    >>> entrypoint, source = translate_as_module("""
    ... 
    ... def g(n):
    ...     i = 0
    ...     while n:
    ...         i = i + n
    ...         n = n - 1
    ...     return i
    ... 
    ... """)
    
This call has invoked a PyPy interpreter running on FlowObjspace, recorded every
possible codepath into a flowgraph, and then rendered the following source code::
    
    >>> print source
    #!/bin/env python
    # -*- coding: LATIN-1 -*-

    def initapp2interpexec(space):
      """NOT_RPYTHON"""

      def g(space, __args__):
        funcname = "g"
        signature = ['n'], None, None
        defaults_w = []
        w_n_2, = __args__.parse(funcname, signature, defaults_w)
        return fastf_g(space, w_n_2)

      f_g = g

      def g(space, w_n_2):
        goto = 3 # startblock
        while True:

            if goto == 1:
                v0 = space.is_true(w_n)
                if v0 == True:
                    w_n_1, w_0 = w_n, w_i
                    goto = 2
                else:
                    assert v0 == False
                    w_1 = w_i
                    goto = 4

            if goto == 2:
                w_2 = space.add(w_0, w_n_1)
                w_3 = space.sub(w_n_1, space.w_True)
                w_n, w_i = w_3, w_2
                goto = 1
                continue

            if goto == 3:
                w_n, w_i = w_n_2, space.w_False
                goto = 1
                continue

            if goto == 4:
                return w_1

      fastf_g = g

      g3dict = space.newdict([])
      gs___name__ = space.wrap('__name__')
      gs_app2interpexec = space.wrap('app2interpexec')
      space.setitem(g3dict, gs___name__, gs_app2interpexec)
      gs_g = space.wrap('g')
      from pypy.interpreter import gateway
      gfunc_g = space.wrap(gateway.interp2app(f_g, unwrap_spec=[gateway.ObjSpace, gateway.Arguments]))
      space.setitem(g3dict, gs_g, gfunc_g)
      return g3dict

You see that actually a single function is produced: 'initapp2interpexec'. This is the
function that you will call with a space as argument. It defines a few functions and then
does a number of initialization steps, builds the global objects the function need,
and produces the interface function gfunc_g to be called from interpreter level.

The return value is ``g3dict``, which contains a module name and the function we asked for.

Let's have a look at the body of this code: The first definition of ``g`` is just
for the argument parsing and is used as ``f_g`` in the gateway.interp2app.
We look at the second definition, ``fastf_g``, which does the actual
computation. Comparing to the flowgraph from above_,
you see a code block for every block in the graph.
Since Python has no goto statement, the jumps between the blocks are implemented
by a loop that switches over a ``goto`` variable.

::

    .       if goto == 1:
                v0 = space.is_true(w_n)
                if v0 == True:
                    w_n_1, w_0 = w_n, w_i
                    goto = 2
                else:
                    assert v0 == False
                    w_1 = w_i
                    goto = 4

This is the implementation of the "``while n:``". There is no implicit state,
everything is passed over to the next block by initializing its
input variables. This directly resembles the nature of flowgraphs.
They are completely stateless.


::

    .       if goto == 2:
                w_2 = space.add(w_0, w_n_1)
                w_3 = space.sub(w_n_1, space.w_True)
                w_n, w_i = w_3, w_2
                goto = 1
                continue

The "``i = i + n``" and "``n = n - 1``" instructions.
You see how every instruction produces a new variable.
The state is again shuffled around by assigning to the
input variables ``w_n`` and ``w_i`` of the next target, block 1.

Note that it is possible to rewrite this by re-using variables,
trying to produce nested blocks instead of the goto construction
and much more. The source would look much more like what we
used to write by hand. For the C backend, this doesn't make much
sense since the compiler optimizes it for us. For the Python interpreter it could
give a bit more speed. But this is a temporary format and will
get optimized anyway when we produce the executable.


How it works
------------

XXX to be added later

