=======================
JIT Generation in PyPy
=======================

Introduction
=============

.. Interpreters to compilers, psyco parallel...

One the of the central goals of the PyPy project is to automatically
produce a Just in Time Compiler from the interpreter, with as little
as possible intervention on the interpreter codebase itself.  The Just
in Time Compiler should be another aspect as much as possible
transparently introduced by and during the translation process.

Partial evaluation techniques should, at least theoretically, allow
such a derivation of a compiler from an interpreter. (XXX references)

The forest of flow graphs that the translation process generates and
transforms constitutes a reasonable base for the necessary analyses.
That's a further reason why having an high-level runnable and
analysable description of the language was always a central tenet of
the project.

Transforming an interpreter into a compiler involves constructing a so
called generating extension, which takes input programs to the
interpreter, and produces what would be the output of partial
evaluating the interpreter with the input data left variable and the
input program being fixed. The generating extension is essentially
capable of compiling the input programs.

Generating extensions can be produced by self-applying partial evaluators,
but this approach may lead to not optimal results or be not scalable
(XXX expand this argument)

For PyPy, our approach aims at producing the generating extension more
directly from the analysed interpreter in the form of a forest of flow
graphs. We call such process *timeshifting*.

To be able to achieve this, gathering binding time information is
crucial, this means for an input program distinguishing values in the
data-flow that are compile-time bound and immutable at run-time,
versus respectively runtime values.

Currently we base the binding time computation on propagating the
information based on a few hint inserted in the interpreter. Propagation
is implemented by reusing our `annotation/type inference framework`__.

__ annotator_

The code produced by a generating extension for an input program may
not be good, especially for a dynamic language, because essentially
the input program doesn't contain enough information to generate good
code. What is really desired is not a generating extension doing
static compilation, but one capable of dynamic compilation, exploiting
runtime information in its result, or using a different terminology
(XXX explain differently the following)
capable of producing code that is specialised with respect to some of
the run-time information, for example language-level types.

Inspired by Psyco, which in some sense is such a specialising
generating extension for Python, but hand-written, we added support
for so-called *promotion* to our framework for producing generating
extensions.

Simply put, promotion on a value stops compilation when encountered,
when the same point is reached at run-time compilation is restarted
and the current run-time value is used and propagated as a compile-time
value. Concretely the promotion expands into a switch to choose based
on the run-time value one of the possible specialised code paths, plus
a default case to compile further specialised paths. This can be also
thought as a generalisation of polymorphic inline caches (XXX reference).


Terminology
=========================================

Partial evaluation is the process of evaluating a function, say ``f(x,
y)``, with only partial information about the value of its arguments,
say the value of the ``x`` argument only.  This produces a *residual*
function ``g(y)``, which takes less arguments than the original - only
the information not specified during the partial evaluation process need
to be provided to the residual function, in this example the ``y``
argument.

Partial evaluation (PE) comes in two flavors:

* *On-line* PE: a compiler-like algorithm takes the source code of the
  function ``f(x, y)`` (or its intermediate representation, i.e. its
  control flow graph in PyPy's terminology), and some partial
  information, e.g. ``x = 5``.  From this, it produces the residual
  function ``g(y)`` directly, by following in which operations the
  knowledge ``x = 5`` can be used, which loops can be unrolled, etc.

* *Off-line* PE: in many cases, the goal of partial evaluation is to
  improve performance in a specific application.  Assume that we have a
  single known function ``f(x, y)`` in which we think that the value of
  ``x`` will change slowly during the execution of our program - much
  more slowly than the value of ``y``.  An obvious example is a loop
  that calls ``f(x, y)`` many times with always the same value ``x``.
  We could then use an on-line partial evaluator to produce a ``g(y)``
  for each new value of ``x``.  In practice, the overhead of the partial
  evaluator might be too large for it to be executed at run-time.
  However, if we know the function ``f`` in advance, and if we know
  *which* arguments are the ones that we will want to partially evaluate
  ``f`` with, then we do not need a full compiler-like analysis of ``f``
  every time the value of ``x`` changes.  We can precompute off-line a
  specialized function ``f1(x)``, which when called produces a residual
  function ``g(y)``.

Off-line partial evaluation is based on *binding-time analysis*, which
is the process of determining among the variables used in a function (or
a set of functions) which ones are going to be known in advance and which
ones are not.  In the above example, such an analysis would be able to
infer that the constantness of the argument ``x`` implies the
constantness of many intermediate values used in the function.  The
*binding time* of a variable determines how early the value of the
variable will be known.

The PyPy JIT is generated using off-line partial evaluation.  As such,
there are three distinct phases:

* *Translation time*: during the normal translation of an RPython
  program like PyPy, we perform binding-time analysis and off-line
  specialization.  This produces a new set of functions (``f1(x)`` in
  our running example) which are linked with the rest of the program, as
  described in Timeshifting_.

* *Compile time*: during the execution of the program, when a new value
  for ``x`` is found, ``f1(x)`` is invoked.  All the computations
  performed by ``f1(x)`` are called compile-time computations.  This is
  justified by the fact that ``f1(x)`` is in some sense a compiler,
  whose sole effect is to produce residual code.

* *Run time*: the normal execution of the program.

The binding-time terminology that we are using in PyPy is based on the
colors that we use when displaying the control flow graphs:

* *Green* variables contain values that are known at compile-time -
  e.g. ``x``.

* *Red* variables contain values that are not known until run-time -
  e.g. ``y``.


Binding-time analysis
=========================================

PyPy performs binding-time analysis of the source RPython_ program after
it has been turned to `low-level graphs`_, i.e. at the level at which
operations manipulate `pointer-and-structures-like objects`_.

The binding-time analyzer of our translation tool-chain is based on the
same type inference engine that is used on the source RPython program,
the annotator_.  In this mode, it is called the *hint-annotator*; it
operates over input graphs that are already low-level instead of
RPython-level, and propagates annotations that do not track types but
value dependencies and manually-provided binding time hints.

Hints
----------------------------------

Our goal in designing our approach to binding-time analysis was to
minimize the number of explicit hints that the user must provide in the
source of the RPython program.  This minimalism was not pushed to
extremes, though, to keep the hint-annotator reasonably simple.

The driving idea was that hints should be need-oriented.  In a program
like an interpreter, there are a few clear places where it would be
beneficial for a given value to be known at compile-time, i.e. green.
This is where we require the hints to be added.

The normal process of the hint-annotator is to propagate the binding
time (i.e. color) of the variables using the following kind of rules:

* For a foldable operation (i.e. one without side effect and which
  depends only on its argument values), if all arguments are green,
  then the result can be green too.

* Non-foldable operations always produce a red result.

* At join points, where multiple possible values (depending on control
  flow) are meeting into a fresh variable, if any incoming value comes
  from a red variable, the result is red.  Otherwise, the color of the
  result might be green.  We do not make it eagerly green, because of
  the control flow dependency: the residual function is basically a
  constant-folded copy of the source function, so it might retain some
  of the same control flow.  The value that needs to be stored in the
  fresh join variable thus depends on which branches are taken in the
  residual graph.

The hint-annotator assumes that all variables are red by default (with
the exception of constants, which are always green).  It then propagates
annotations that record dependency information.  When encountering the
user-provided hints, the dependency information is used to make some
variables green.  (Technically, the color itself is not part of the
annotations propagated by the annotator.)  All hints are in the form of
an operation ``hint(v1, someflag=True)`` which semantically just returns
its first argument unmodified.  The three kinds of hints that are useful
in practice are:

``v2 = hint(v1, concrete=True)``
    This is interpreted by the hint-annotator as a request for both
    ``v1`` and ``v2`` to be green.  It is used in places where the
    programmer considers the knowledge of the value to be essential.
    This hint has a *global* effect on the binding times: it means that
    not only ``v1`` but all the values that ``v1`` depends on -
    recursively - are forced to be green.  Typically, it can only be
    applied on values that directly depend on some input arguments,
    making these input arguments green.  The hint-annotator complains if
    the dependencies of ``v1`` include a value that cannot be green,
    like a value read out of a field out of a non-immutable structure.

    The color of the result ``v2`` is green as well.  Unlike ``v1``,
    all further operations involving ``v2`` are checked to not meet
    any red variable (i.e. ``v2`` color is eagerly and recursively
    propagated, while ``v1`` is only green and may be involved in
    further operations that will produce red results).

``v2 = hint(v1, promote=True)``
    This hint is a *local* request for ``v2`` to be green.  Unlike the
    previous hint, this one has no effect on the color of ``v1`` (which
    is typically red - the hint has no effect otherwise).

    Note that in classical approaches to partial evaluation, it is not
    possible to copy a red value into a green one.  The implementation
    of such an operation ("promotion") is only possible in a
    "just-in-time" approach that only Psyco_ implemented so far (to the
    best of our knowledge).  Our hint is a direct generalization of the
    latter.

``v2 = hint(v1, variable=True)``
    Force ``v2`` to be red, even if ``v1`` is green.

A program using promotion also needs to contain a ``global_merge_point``
hint; this has no effect on the hint-annotator and is described in the
section about promotion_.


.. _`example above`:

Example
----------------------------------

Let's consider a very small interpreter-like example::

        def ll_plus_minus(s, x, y):
            acc = x
            pc = 0
            while pc < len(s):
                op = s[pc]
                op = hint(op, concrete=True)
                if op == '+':
                    acc += y
                elif op == '-':
                    acc -= y
                pc += 1
            return acc

``s`` here is an input program, simply a string of "+" or "-", x and y
are integer input arguments.

The annotation of ``op = hint(op, concrete=True)`` will follow the
dependencies of the argument ``op``, which is the result of ``op =
s[pc]``, so both ``s``and ``pc`` will be marked green. ``x``, ``y``
and ``acc`` will stay red.

The result ``op`` compared to the possible "instructions" will also be
green. Because ``s``is green also ``len(s)`` will be.

Using this information the *timeshifting* of the ``ll_plus_minus``,
should be able to produce code that unfolds the loop and respectively
folds instruction dispatching (the ifs) at compile-time, because the
while condition, the ``pc``increment, and the dispatching are
operations among only greens (constants are green by default).

Calls
----------------------------------

The ``concrete`` hint requires precise tracking of dependencies across
calls.  More specifically, unlike the regular type-inferencing
annotator, the hint-annotator does not simply propagate this information
through called functions, but keeps all dependency information local to a
function.  The problem is best shown by an example::

    x = ...
    y = ...
    z = f(x, y)

In this example, assuming that ``f(x, y)`` has no side effects, then we
want the dependency set of ``z`` to be the union of the dependencies of
``x`` and ``y`` (if both contribute to the result).  But if another call
site calls ``f(x2, y2)`` then precision would be lost - the annotations
that would propagate within ``f`` would be marked as depending on all of
``x``, ``x2``, ``y`` and ``y2``, and then so would ``z``.

To fix this problem, we only propagate dependencies that are local to a
function.  Around a call like ``z = f(x, y)``, we check in the
annotations of the function ``f`` on which input arguments the result is
marked as depending on, and we reproduce the same dependency
relationship locally between ``x`` and ``y`` and ``z``.

There are other aspects of the information carried by annotations that
are used in the "usual" annotator way, i.e. directly propagated between
caller and callee and back.  If the result ``z`` of a call is forced to
be green by a ``concrete`` hint, then this forcing effect is propagated
inside the callee (more precisely, in a fresh copy of the callee, so
that forced and non-forced call sites can respectively call a
mostly-green and a mostly-red version of the function; this is done with
the technique of `specialization of functions`_ that is already
available in the normal annotator).

Also, if all call sites provide a green value for an argument, then the
callee's corresponding input argument can safely be green.  (This can
only be determined at the end of the process, though, after all call
sites are known and stable enough; we use another fixpoint loop for
this.)


Deep freezing
----------------------------------

Among the low-level operations, the one that reads data from a structure
in memory (``getfield``) requires special care.  In a first
approximation it can only return a red value, because the reading from
the structure cannot be performed at compile-time - the structure may be
mutated between compile-time and the point at run-time where the read
was supposed to occur in the original code.

This is a problem in most real-life examples.  Unlike our `example
above`_, input arguments to such functions are typically not just
strings and integers but complex data structures.  To produce sensible
results, it is necessary to assume that some of these data structures
will not be mutated after the compile-time process used their content.
For example, PyPy's own interpreter works with an instance of a PyCode
class containing not only a string representing the bytecode, but
various simple Python objects (tuple of names, ...).  None of this data
can be modified after its construction, though.

To express this, we use a hint ``v2 = hint(v1, deepfreeze=True)``, where
``v1`` is a pointer (in the low-level graph - it typically comes from a
regular RPython object reference in the RPython source).  The
hint-annotation will then propagate a "deepfrozen" flag on the
annotation attached to ``v2``.  If ``v2`` is green, a ``getfield(v2,
"name")`` operation then also returns a green.  Moreover, the result of
the ``getfield`` is also itself "deepfrozen".  We decided to implement
recursive freezing and not one-level-only freezing, as the latter seems
more fragile with respect to changes both in the RPython source and in
the way the source maps to low-level graphs; but variants could easily
be implemented if needed.


Blue containers
----------------------------------

Undocumented yet.  Not implemented in the timeshifter, so not used so far.



.. _timeshifting:

Timeshifting: transforming interpreter into compilers
======================================================

intro and basics

Basics
--------

...red vs green operations...

XXX Note that the "deepfrozen" flag is used at compile-time on red
values too, to influence the details of how residual ``getfield``
operations should be generated.

Transform vs hrtyping
-----------------------

...

Split and Merges
--------------------

...

Calls and inlining
---------------------

...

Virtual Containers
--------------------

...vstructs, vlist, vdict...

Exceptions
------------

...

.. _promotion:

Promotion and global merges
-----------------------------

...global merge point...

Partial data
-------------

...

Portals
----------------------

...

Scaling to PyPy
----------------------

...


Backends
====================

...

The Backend interface
-----------------------

...


.. _`RPython`: coding-guide.html#rpython
.. _`low-level graphs`: rtyper.html
.. _`pointer-and-structures-like objects`: rtyper.html#low-level-types 
.. _`annotator`: dynamic-language-translation.html
.. _`specialization of functions`: dynamic-language-translation.html#specialization
.. _Psyco: http://psyco.sourceforge.net
