
1b664888133d (March 15, 2014): the overhead of a non-JIT STM, when
compared with a non-JIT plain PyPy, is measured to be 54% in a
single-threaded trivial benchmark.  A tentative break-down of this figure:

* 15% are from stm_write() on writing non-GC pointers into GC objects

* 7% are from stm_write() on writing a constant GC pointer (likely null)
  (in a regular pypy, this doesn't emit a write_barrier)

* 14% are from stm_read()

* 3% (in this benchmark) is just a slower startup time

* 6% where removed soon afterwards by ddbc16971682

* the rest: ~9% from unknown other places (may include accessing
  prebuilt GC objects, which requires an indirection)

UPDATE: with ddbc16971682 the figure seems to be: only 38% slower.
Assuming that all other points stayed at the same overhead, it would
perfectly explain the slow-down.

------------------------------------------------------------

clang doesn't optimize multiple stm_write() in a row (unlike GCC).
Optimize them manually...

------------------------------------------------------------

__pypy__.thread.getsegmentlimit():

XXX This limit is so far a compile time option (STM_NB_SEGMENTS in
rpython/translator/stm/src_stm/stmgc.h), but this should instead be
based on the machine found at run-time.  We should also be able to
change the limit (or at least lower it) with setsegmentlimit().

------------------------------------------------------------

JIT: add an artificial malloc if the loop is so small as to contain
any!

------------------------------------------------------------

weakrefs stay alive longer than expected::
    
    y = some object that was already alive for a while
    x = ref(y)
    del y
    gc.collect()        # y doesn't die: it's needed if we abort
    assert x() is None  # so this assert fails

A dying weakref might be a cross-transaction way to exchange
information when there should be none::

    thread 1:
        if <condition>:
            x = some_weakref()
        ...

    thread 2:
        gc.collect()   # will kill some_weakref() but only if
                       # thread 1 did not, so far, read it
        if some_weakref() is None:
            ...

It might be enough to apply these rules: (1) an explicit gc.collect()
turns the transaction inevitable first; (2) if any non-inevitable
transaction has *read* the weakref yet, then its target remains alive.

This might require a tweak to consider an object as dead (for the
purposes of weakrefs) if it's only reachable via the old version of an
old_modified_object in the inevitable transaction: in this case,
other transaction may still reach the objects in question, so it
shouldn't be deallocted just now, but by doing so they will put
themselves in a situation where they necessarily abort.

------------------------------------------------------------

missing recursion detection (both in interpreted and JITted mode)

------------------------------------------------------------

change the limit of 1 GB

------------------------------------------------------------

Re-add the disabled optimization (only outside the jit):
(1) withmethodcache
(2) LOAD_ATTR_caching, LOOKUP_METHOD_mapdict

------------------------------------------------------------

pypy_g_BlackholeInterpBuilder_acquire_interp creates conflicts
by caching the BlackholeInterps

------------------------------------------------------------




===============================================================================
===========    the rest is from stmgc-c4    ===================================
===============================================================================



------------------------------------------------------------

POSSIBLE BUG:
part of this is done: still investigate where transaction
  breaks are really allowed to happen in the JIT. (JUMP,
  FINISH, call_footer(), ...)
investigate if another thread can force a jitframe. Thus,
making a transaction break *after* a guard_not_forced
would be wrong, as the force will only be visible after
the break. (The GIL doesn't get released inbetween
the GUARD and the next call that is allowed to, so no
problems there)..
Solution, move transaction breaks right before guard_not_forced, maybe

------------------------------------------------------------

should stm_thread_local_obj always be read & writeable? would
a write-barrier in begin_transaction be too much for small
transactions? should we handle it specially (undolog?)

------------------------------------------------------------

looking at trace of fibo.tlc (targettlc.py), there are a lot
of read-barriers followed by write-barriers. Merging them
and making the first a write-barrier is important!
(attention: changing A2R->A2W after having placed the barrier
must still invalidate all A2R at the point of the new A2W)

------------------------------------------------------------

we have crashes when setting trace_eagerness very low
-> many guards generated.
It may be because patching the assembler code is not atomic.
(unlikely after trying to (badly) synchronize code around
places that patch assembler)
Maybe solve by new stmgc library function that synchronizes
all threads so only the caller is running.

------------------------------------------------------------

stmgc-library: since we copy back over h_originals, it may
make sense to treat them differently during allocation and
collection.
E.g. have a separate space to allocate h_originals from.
Thus, we have a more compact layout in memory (fragmentation).

------------------------------------------------------------

make stm_transaction_break use cond_call (or other ways to not
spill all registers)

------------------------------------------------------------

constptrs always require slowpath of read_barrier if they
point to a stub
they also always require the slowpath of a write-barrier
because there is always one indirection to the current version

------------------------------------------------------------

we may have too many transaction breaks in jitted code.

------------------------------------------------------------

unregister constptrs in stmgc when freeing traces

------------------------------------------------------------

stm-jitdriver with autoreds

------------------------------------------------------------

try to let non-atomic inevitable transactions run for longer, until
another thread starts waiting for the mutex

------------------------------------------------------------

RPyAssert(i < len(lst)): if lst is global this turns into tons of code

------------------------------------------------------------

GC: major collections; call __del__()

------------------------------------------------------------

JIT: finish (missing: the call in execute_token(), reorganize pypy source, ?)

------------------------------------------------------------

optimize the static placement of the STM_XxxBARRIERs and use them in JIT

------------------------------------------------------------



Current optimization opportunities (outside the JIT)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

tweak translator/stm/ to improve placement of barriers, at least at
whole-function level, but maybe cross-function; and reintroduce tweaks
to the PyFrame object (make sure it's always written and don't put more
barriers)

in parallel, tweak the API of stmgc: support "tentative" write_barrier calls
that are not actually followed by a write (checked by comparing the
object contents)

in the interpreter, e.g. BINARY_ADD calls space.add() which possibly
(but rarely) can cause a transaction break, thus requiring that the
frame be write-barrier()-ed again.  I'm thinking about alternatives for
this case: e.g. have a separate stack of objects, and the top-most
object on this stack is always in write mode.  so just after a
transaction break, we force a write barrier on the top object of the
stack.  this would be needed to avoid the usually-pointless write
barriers on the PyFrame everywhere in the interpreter

running valgrind we can see X% of the time in the read or write
barriers, but it would be interesting to know also the time spent in the
fast-path, as well as splitting it based e.g. on the RPython type of
object.  See also vtune.

JIT
~~~

* use specialized barriers in JIT
* optimize produced assembler code
* avoid calling aroundstate.after() for call_release_gil and instead
  start a normal transaction after the call
* maybe GUARD_NOT_INEVITABLE after call_may_force, call_assembler
  which is a small check if we are inevitable and does a transaction_break
  if we are.
* look at XXXs for STM everywhere
